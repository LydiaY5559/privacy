{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pate2017single.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasiqrumaney/privacy/blob/master/notebooks/pate2017single.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GovDUv_4rmZ0",
        "colab_type": "text"
      },
      "source": [
        "###Creating directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeXV2eZVcKdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!mkdir models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiV7xWvEryi1",
        "colab_type": "text"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHCqrf1Hr3_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "# !pip install -q tf-nightly-2.0-preview\n",
        "import gzip\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io import loadmat as loadmat\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import urllib\n",
        "from six.moves import xrange\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from datetime import datetime as dt\n",
        "import time\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNGjnBY0zd0r",
        "colab_type": "text"
      },
      "source": [
        "### Flags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnpMJ1E-zcyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = 'mnist'\n",
        "epochs_per_decay = 350\n",
        "nb_teachers = 10\n",
        "# teacher_id = 0\n",
        "dropout_seed = 123\n",
        "batch_size = 128\n",
        "nb_labels = 10\n",
        "deeper = False\n",
        "batch_size = 128\n",
        "max_steps = 3000\n",
        "log_device_placement = False\n",
        "learning_rate = 5\n",
        "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
        "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
        "train_dir = '/content/models'\n",
        "data_dir = '/content/data'\n",
        "\n",
        "stdnt_share = 1000\n",
        "lap_scale = 10\n",
        "teachers_dir = '/content/models'\n",
        "teachers_max_steps = 3000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKjgCRnb07F2",
        "colab_type": "text"
      },
      "source": [
        "### Code from utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qa51ppr041i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_indices(batch_nb, data_length, batch_size):\n",
        "  \"\"\"\n",
        "  This helper function computes a batch start and end index\n",
        "  :param batch_nb: the batch number\n",
        "  :param data_length: the total length of the data being parsed by batches\n",
        "  :param batch_size: the number of inputs in each batch\n",
        "  :return: pair of (start, end) indices\n",
        "  \"\"\"\n",
        "  # Batch start and end index\n",
        "  start = int(batch_nb * batch_size)\n",
        "  end = int((batch_nb + 1) * batch_size)\n",
        "\n",
        "  # When there are not enough inputs left, we reuse some to complete the batch\n",
        "  if end > data_length:\n",
        "    shift = end - data_length\n",
        "    start -= shift\n",
        "    end -= shift\n",
        "\n",
        "  return start, end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCttCPwxDpS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(logits, labels):\n",
        "  \"\"\"\n",
        "  Return accuracy of the array of logits (or label predictions) wrt the labels\n",
        "  :param logits: this can either be logits, probabilities, or a single label\n",
        "  :param labels: the correct labels to match against\n",
        "  :return: the accuracy as a float\n",
        "  \"\"\"\n",
        "  assert len(logits) == len(labels)\n",
        "\n",
        "  if len(np.shape(logits)) > 1:\n",
        "    # Predicted labels are the argmax over axis 1\n",
        "    predicted_labels = np.argmax(logits, axis=1)\n",
        "  else:\n",
        "    # Input was already labels\n",
        "    assert len(np.shape(logits)) == 1\n",
        "    predicted_labels = logits\n",
        "\n",
        "  # Check against correct labels to compute correct guesses\n",
        "  correct = np.sum(predicted_labels == labels.reshape(len(labels)))\n",
        "\n",
        "  # Divide by number of labels to obtain accuracy\n",
        "  accuracy = float(correct) / len(labels)\n",
        "\n",
        "  # Return float value\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uMyjaLHsbcR",
        "colab_type": "text"
      },
      "source": [
        "###  Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH5AOMxzqY2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_urls = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
        "               'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
        "               'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
        "               'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
        "              ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U0IkLGjsGNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maybe_download(file_urls, directory):\n",
        "  \"\"\"Download a set of files in temporary local folder.\"\"\"\n",
        "\n",
        "  # This list will include all URLS of the local copy of downloaded files\n",
        "  result = []\n",
        "\n",
        "  # For each file of the dataset\n",
        "  for file_url in file_urls:\n",
        "    # Extract filename\n",
        "    filename = file_url.split('/')[-1]\n",
        "\n",
        "    # If downloading from GitHub, remove suffix ?raw=True from local filename\n",
        "    if filename.endswith(\"?raw=true\"):\n",
        "      filename = filename[:-9]\n",
        "\n",
        "    # Deduce local file url\n",
        "    #filepath = os.path.join(directory, filename)\n",
        "    filepath = directory + '/' + filename\n",
        "\n",
        "    # Add to result list\n",
        "    result.append(filepath)\n",
        "\n",
        "    # Test if file already exists\n",
        "    if not tf.gfile.Exists(filepath):\n",
        "      def _progress(count, block_size, total_size):\n",
        "        sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
        "            float(count * block_size) / float(total_size) * 100.0))\n",
        "        sys.stdout.flush()\n",
        "      filepath, _ = urllib.request.urlretrieve(file_url, filepath, _progress)\n",
        "      print()\n",
        "      statinfo = os.stat(filepath)\n",
        "      print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEOBwWbstRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f98e9bf9-90ab-48cb-ef12-84368e44ae77"
      },
      "source": [
        "local_urls = maybe_download(file_urls, '/content/data')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Downloading train-images-idx3-ubyte.gz 100.1%\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            ">> Downloading train-labels-idx1-ubyte.gz 113.5%\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            ">> Downloading t10k-images-idx3-ubyte.gz 100.4%\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            ">> Downloading t10k-labels-idx1-ubyte.gz 180.4%\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgV_kSpHtqpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_mnist_data(filename, num_images, image_size, pixel_depth):\n",
        "  \"\"\"\n",
        "  Extract the images into a 4D tensor [image index, y, x, channels].\n",
        "\n",
        "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
        "  \"\"\"\n",
        "  if not tf.gfile.Exists(filename+'.npy'):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "      bytestream.read(16)\n",
        "      buf = bytestream.read(image_size * image_size * num_images)\n",
        "      data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "      data = (data - (pixel_depth / 2.0)) / pixel_depth\n",
        "      data = data.reshape(num_images, image_size, image_size, 1)\n",
        "      np.save(filename, data)\n",
        "      return data\n",
        "  else:\n",
        "    with tf.gfile.Open(filename+'.npy', mode='rb') as file_obj:\n",
        "      return np.load(file_obj)\n",
        "\n",
        "\n",
        "def extract_mnist_labels(filename, num_images):\n",
        "  \"\"\"\n",
        "  Extract the labels into a vector of int64 label IDs.\n",
        "  \"\"\"\n",
        "  if not tf.gfile.Exists(filename+'.npy'):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "      bytestream.read(8)\n",
        "      buf = bytestream.read(1 * num_images)\n",
        "      labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int32)\n",
        "      np.save(filename, labels)\n",
        "    return labels\n",
        "  else:\n",
        "    with tf.gfile.Open(filename+'.npy', mode='rb') as file_obj:\n",
        "      return np.load(file_obj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzuF-jCSt8PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract it into np arrays.\n",
        "train_data = extract_mnist_data(local_urls[0], 60000, 28, 1)\n",
        "train_labels = extract_mnist_labels(local_urls[1], 60000)\n",
        "test_data = extract_mnist_data(local_urls[2], 10000, 28, 1)\n",
        "test_labels = extract_mnist_labels(local_urls[3], 10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hprJ-5oNuAmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def partition_dataset(data, labels, nb_teachers, teacher_id):\n",
        "  \"\"\"\n",
        "  Simple partitioning algorithm that returns the right portion of the data\n",
        "  needed by a given teacher out of a certain nb of teachers\n",
        "\n",
        "  Args:\n",
        "    data: input data to be partitioned\n",
        "    labels: output data to be partitioned\n",
        "    nb_teachers: number of teachers in the ensemble (affects size of each\n",
        "                      partition)\n",
        "    teacher_id: id of partition to retrieve\n",
        "  \"\"\"\n",
        "\n",
        "  # Sanity check\n",
        "  assert len(data) == len(labels)\n",
        "  assert int(teacher_id) < int(nb_teachers)\n",
        "\n",
        "  # This will floor the possible number of batches\n",
        "  batch_len = int(len(data) / nb_teachers)\n",
        "\n",
        "  # Compute start, end indices of partition\n",
        "  start = teacher_id * batch_len\n",
        "  end = (teacher_id+1) * batch_len\n",
        "\n",
        "  # Slice partition off\n",
        "  partition_data = data[start:end]\n",
        "  partition_labels = labels[start:end]\n",
        "\n",
        "  return partition_data, partition_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V42M5EKZONjX",
        "colab_type": "text"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1DQ6Z7YOdxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete any old logs.... be smart while using this\n",
        "% rm -rf /content/logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuMs_zF_yjV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c82b3b0-072a-4bf4-ddd0-11ee680c826b"
      },
      "source": [
        "# Install latest Tensorflow build\n",
        "!pip install -q tf-nightly-2.0-preview\n",
        "import tensorflow as tf\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard module is not an IPython extension.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5yhxAkiOleF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% mkdir -p '/content/logs/tensorboard/teacher/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqjdT2udO_2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% mkdir -p '/content/logs/tensorboard/student/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v29Gv0nuOzPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "31f30973-3635-47cf-ef71-f1f4904f7c71"
      },
      "source": [
        "import datetime\n",
        "current_time = str(dt.now().timestamp())\n",
        "teacher_log_dir = '/content/logs/tensorboard/teacher/' + current_time\n",
        "student_log_dir = '/content/logs/tensorboard/student/' + current_time\n",
        "teacher_summary_writer = summary.FileWriter(teacher_log_dir)\n",
        "student_summary_writer = summary.FileWriter(student_log_dir)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d43dac14efb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mteacher_log_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/logs/tensorboard/teacher/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstudent_log_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/logs/tensorboard/student/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mteacher_summary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_log_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstudent_summary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_log_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'summary' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEfTvzKAPRwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIulZ9vrOSTB",
        "colab_type": "text"
      },
      "source": [
        "### DeepCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE5xycGy_haw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _variable_on_cpu(name, shape, initializer):\n",
        "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
        "\n",
        "  Args:\n",
        "    name: name of the variable\n",
        "    shape: list of ints\n",
        "    initializer: initializer for Variable\n",
        "\n",
        "  Returns:\n",
        "    Variable Tensor\n",
        "  \"\"\"\n",
        "  with tf.device('/cpu:0'):\n",
        "    var = tf.get_variable(name, shape, initializer=initializer)\n",
        "  return var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEnBdvWXy2at",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
        "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
        "\n",
        "  Note that the Variable is initialized with a truncated normal distribution.\n",
        "  A weight decay is added only if one is specified.\n",
        "\n",
        "  Args:\n",
        "    name: name of the variable\n",
        "    shape: list of ints\n",
        "    stddev: standard deviation of a truncated Gaussian\n",
        "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "        decay is not added for this Variable.\n",
        "\n",
        "  Returns:\n",
        "    Variable Tensor\n",
        "  \"\"\"\n",
        "  var = _variable_on_cpu(name, shape,\n",
        "                         tf.truncated_normal_initializer(stddev=stddev))\n",
        "  if wd is not None:\n",
        "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "    tf.add_to_collection('losses', weight_decay)\n",
        "  return var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV68edrZB5Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(images, dropout=False):\n",
        "  \"\"\"Build the CNN model.\n",
        "  Args:\n",
        "    images: Images returned from distorted_inputs() or inputs().\n",
        "    dropout: Boolean controlling whether to use dropout or not\n",
        "  Returns:\n",
        "    Logits\n",
        "  \"\"\"\n",
        "  first_conv_shape = [5, 5, 1, 64]\n",
        "\n",
        "  # conv1\n",
        "  with tf.variable_scope('conv1') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=first_conv_shape,\n",
        "                                         stddev=1e-4,\n",
        "                                         wd=0.0)\n",
        "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv1 = tf.nn.relu(bias, name=scope.name)\n",
        "    if dropout:\n",
        "      conv1 = tf.nn.dropout(conv1, 0.3, seed=dropout_seed)\n",
        "\n",
        "\n",
        "  # pool1\n",
        "  pool1 = tf.nn.max_pool(conv1,\n",
        "                         ksize=[1, 3, 3, 1],\n",
        "                         strides=[1, 2, 2, 1],\n",
        "                         padding='SAME',\n",
        "                         name='pool1')\n",
        "\n",
        "  # norm1\n",
        "  norm1 = tf.nn.lrn(pool1,\n",
        "                    4,\n",
        "                    bias=1.0,\n",
        "                    alpha=0.001 / 9.0,\n",
        "                    beta=0.75,\n",
        "                    name='norm1')\n",
        "\n",
        "  # conv2\n",
        "  with tf.variable_scope('conv2') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=[5, 5, 64, 128],\n",
        "                                         stddev=1e-4,\n",
        "                                         wd=0.0)\n",
        "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "    biases = _variable_on_cpu('biases', [128], tf.constant_initializer(0.1))\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv2 = tf.nn.relu(bias, name=scope.name)\n",
        "    if dropout:\n",
        "      conv2 = tf.nn.dropout(conv2, 0.3, seed=dropout_seed)\n",
        "\n",
        "\n",
        "  # norm2\n",
        "  norm2 = tf.nn.lrn(conv2,\n",
        "                    4,\n",
        "                    bias=1.0,\n",
        "                    alpha=0.001 / 9.0,\n",
        "                    beta=0.75,\n",
        "                    name='norm2')\n",
        "\n",
        "  # pool2\n",
        "  pool2 = tf.nn.max_pool(norm2,\n",
        "                         ksize=[1, 3, 3, 1],\n",
        "                         strides=[1, 2, 2, 1],\n",
        "                         padding='SAME',\n",
        "                         name='pool2')\n",
        "\n",
        "  # local3\n",
        "  with tf.variable_scope('local3') as scope:\n",
        "    # Move everything into depth so we can perform a single matrix multiply.\n",
        "    reshape = tf.reshape(pool2, [batch_size, -1])\n",
        "    dim = reshape.get_shape()[1].value\n",
        "    weights = _variable_with_weight_decay('weights',\n",
        "                                          shape=[dim, 384],\n",
        "                                          stddev=0.04,\n",
        "                                          wd=0.004)\n",
        "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
        "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
        "    if dropout:\n",
        "      local3 = tf.nn.dropout(local3, 0.5, seed=dropout_seed)\n",
        "\n",
        "  # local4\n",
        "  with tf.variable_scope('local4') as scope:\n",
        "    weights = _variable_with_weight_decay('weights',\n",
        "                                          shape=[384, 192],\n",
        "                                          stddev=0.04,\n",
        "                                          wd=0.004)\n",
        "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
        "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
        "    if dropout:\n",
        "      local4 = tf.nn.dropout(local4, 0.5, seed=dropout_seed)\n",
        "\n",
        "  # compute logits\n",
        "  with tf.variable_scope('softmax_linear') as scope:\n",
        "    weights = _variable_with_weight_decay('weights',\n",
        "                                          [192, nb_labels],\n",
        "                                          stddev=1/192.0,\n",
        "                                          wd=0.0)\n",
        "    biases = _variable_on_cpu('biases',\n",
        "                              [nb_labels],\n",
        "                              tf.constant_initializer(0.0))\n",
        "    logits = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
        "\n",
        "  return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TafRCATMzE1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_deeper(images, dropout=False):\n",
        "  \"\"\"Build a deeper CNN model.\n",
        "  Args:\n",
        "    images: Images returned from distorted_inputs() or inputs().\n",
        "    dropout: Boolean controlling whether to use dropout or not\n",
        "  Returns:\n",
        "    Logits\n",
        "  \"\"\"\n",
        "  if dataset == 'mnist':\n",
        "    first_conv_shape = [3, 3, 1, 96]\n",
        "  else:\n",
        "    first_conv_shape = [3, 3, 3, 96]\n",
        "\n",
        "  # conv1\n",
        "  with tf.variable_scope('conv1') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=first_conv_shape,\n",
        "                                         stddev=0.05,\n",
        "                                         wd=0.0)\n",
        "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "    biases = _variable_on_cpu('biases', [96], tf.constant_initializer(0.0))\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv1 = tf.nn.relu(bias, name=scope.name)\n",
        "\n",
        "  # conv2\n",
        "  with tf.variable_scope('conv2') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=[3, 3, 96, 96],\n",
        "                                         stddev=0.05,\n",
        "                                         wd=0.0)\n",
        "    conv = tf.nn.conv2d(conv1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "    biases = _variable_on_cpu('biases', [96], tf.constant_initializer(0.0))\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv2 = tf.nn.relu(bias, name=scope.name)\n",
        "\n",
        "  # conv3\n",
        "  with tf.variable_scope('conv3') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=[3, 3, 96, 96],\n",
        "                                         stddev=0.05,\n",
        "                                         wd=0.0)\n",
        "    conv = tf.nn.conv2d(conv2, kernel, [1, 2, 2, 1], padding='SAME')\n",
        "    biases = _variable_on_cpu('biases', [96], tf.constant_initializer(0.0))\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv3 = tf.nn.relu(bias, name=scope.name)\n",
        "    if dropout:\n",
        "      conv3 = tf.nn.dropout(conv3, 0.5, seed=dropout_seed)\n",
        "\n",
        "  # conv4\n",
        "  with tf.variable_scope('conv4') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=[3, 3, 96, 192],\n",
        "                                         stddev=0.05,\n",
        "                                         wd=0.0)\n",
        "    conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.0))\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv4 = tf.nn.relu(bias, name=scope.name)\n",
        "\n",
        "  # conv5\n",
        "  with tf.variable_scope('conv5') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=[3, 3, 192, 192],\n",
        "                                         stddev=0.05,\n",
        "                                         wd=0.0)\n",
        "    conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.0))\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv5 = tf.nn.relu(bias, name=scope.name)\n",
        "\n",
        "  # conv6\n",
        "  with tf.variable_scope('conv6') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=[3, 3, 192, 192],\n",
        "                                         stddev=0.05,\n",
        "                                         wd=0.0)\n",
        "    conv = tf.nn.conv2d(conv5, kernel, [1, 2, 2, 1], padding='SAME')\n",
        "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.0))\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv6 = tf.nn.relu(bias, name=scope.name)\n",
        "    if dropout:\n",
        "      conv6 = tf.nn.dropout(conv6, 0.5, seed=dropout_seed)\n",
        "\n",
        "\n",
        "  # conv7\n",
        "  with tf.variable_scope('conv7') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=[5, 5, 192, 192],\n",
        "                                         stddev=1e-4,\n",
        "                                         wd=0.0)\n",
        "    conv = tf.nn.conv2d(conv6, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv7 = tf.nn.relu(bias, name=scope.name)\n",
        "\n",
        "\n",
        "  # local1\n",
        "  with tf.variable_scope('local1') as scope:\n",
        "    # Move everything into depth so we can perform a single matrix multiply.\n",
        "    reshape = tf.reshape(conv7, [batch_size, -1])\n",
        "    dim = reshape.get_shape()[1].value\n",
        "    weights = _variable_with_weight_decay('weights',\n",
        "                                          shape=[dim, 192],\n",
        "                                          stddev=0.05,\n",
        "                                          wd=0)\n",
        "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
        "    local1 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
        "\n",
        "  # local2\n",
        "  with tf.variable_scope('local2') as scope:\n",
        "    weights = _variable_with_weight_decay('weights',\n",
        "                                          shape=[192, 192],\n",
        "                                          stddev=0.05,\n",
        "                                          wd=0)\n",
        "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
        "    local2 = tf.nn.relu(tf.matmul(local1, weights) + biases, name=scope.name)\n",
        "    if dropout:\n",
        "      local2 = tf.nn.dropout(local2, 0.5, seed=dropout_seed)\n",
        "\n",
        "  # compute logits\n",
        "  with tf.variable_scope('softmax_linear') as scope:\n",
        "    weights = _variable_with_weight_decay('weights',\n",
        "                                          [192, nb_labels],\n",
        "                                          stddev=0.05,\n",
        "                                          wd=0.0)\n",
        "    biases = _variable_on_cpu('biases',\n",
        "                              [nb_labels],\n",
        "                              tf.constant_initializer(0.0))\n",
        "    logits = tf.add(tf.matmul(local2, weights), biases, name=scope.name)\n",
        "\n",
        "  return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrJ9OWe2zL4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fun(logits, labels):\n",
        "  \"\"\"Add L2Loss to all the trainable variables.\n",
        "\n",
        "  Add summary for \"Loss\" and \"Loss/avg\".\n",
        "  Args:\n",
        "    logits: Logits from inference().\n",
        "    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
        "            of shape [batch_size]\n",
        "    distillation: if set to True, use probabilities and not class labels to\n",
        "                  compute softmax loss\n",
        "\n",
        "  Returns:\n",
        "    Loss tensor of type float.\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the cross entropy between labels and predictions\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      logits=logits, labels=labels, name='cross_entropy_per_example')\n",
        "\n",
        "  # Calculate the average cross entropy loss across the batch.\n",
        "  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
        "\n",
        "  # Add to TF collection for losses\n",
        "  tf.add_to_collection('losses', cross_entropy_mean)\n",
        "\n",
        "  # The total loss is defined as the cross entropy loss plus all of the weight\n",
        "  # decay terms (L2 loss).\n",
        "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc0XzPLI0iN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def moving_av(total_loss):\n",
        "  \"\"\"\n",
        "  Generates moving average for all losses\n",
        "\n",
        "  Args:\n",
        "    total_loss: Total loss from loss().\n",
        "  Returns:\n",
        "    loss_averages_op: op for generating moving averages of losses.\n",
        "  \"\"\"\n",
        "  # Compute the moving average of all individual losses and the total loss.\n",
        "  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
        "  losses = tf.get_collection('losses')\n",
        "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
        "\n",
        "  return loss_averages_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBOrj8spBoaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_op_fun(total_loss, global_step):\n",
        "  \"\"\"Train model.\n",
        "\n",
        "  Create an optimizer and apply to all trainable variables. Add moving\n",
        "  average for all trainable variables.\n",
        "\n",
        "  Args:\n",
        "    total_loss: Total loss from loss().\n",
        "    global_step: Integer Variable counting the number of training steps\n",
        "      processed.\n",
        "  Returns:\n",
        "    train_op: op for training.\n",
        "  \"\"\"\n",
        "  # Variables that affect learning rate.\n",
        "  nb_ex_per_train_epoch = int(60000 / nb_teachers)\n",
        "\n",
        "  num_batches_per_epoch = nb_ex_per_train_epoch / batch_size\n",
        "  decay_steps = int(num_batches_per_epoch * epochs_per_decay)\n",
        "\n",
        "  initial_learning_rate = float(learning_rate) / 100.0\n",
        "\n",
        "  # Decay the learning rate exponentially based on the number of steps.\n",
        "  lr = tf.train.exponential_decay(initial_learning_rate,\n",
        "                                  global_step,\n",
        "                                  decay_steps,\n",
        "                                  LEARNING_RATE_DECAY_FACTOR,\n",
        "                                  staircase=True)\n",
        "  tf.summary.scalar('learning_rate', lr)\n",
        "\n",
        "  # Generate moving averages of all losses and associated summaries.\n",
        "  loss_averages_op = moving_av(total_loss)\n",
        "\n",
        "  # Compute gradients.\n",
        "  with tf.control_dependencies([loss_averages_op]):\n",
        "    opt = tf.train.GradientDescentOptimizer(lr)\n",
        "    grads = opt.compute_gradients(total_loss)\n",
        "\n",
        "  # Apply gradients.\n",
        "  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
        "\n",
        "  # Add histograms for trainable variables.\n",
        "  for var in tf.trainable_variables():\n",
        "    tf.summary.histogram(var.op.name, var)\n",
        "\n",
        "  # Track the moving averages of all trainable variables.\n",
        "  variable_averages = tf.train.ExponentialMovingAverage(\n",
        "      MOVING_AVERAGE_DECAY, global_step)\n",
        "  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
        "\n",
        "  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
        "    train_op = tf.no_op(name='train')\n",
        "\n",
        "  return train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nyFB4cBlK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _input_placeholder():\n",
        "  \"\"\"\n",
        "  This helper function declares a TF placeholder for the graph input data\n",
        "  :return: TF placeholder for the graph input data\n",
        "  \"\"\"\n",
        "  image_size = 28\n",
        "  num_channels = 1\n",
        "\n",
        "  # Declare data placeholder\n",
        "  train_node_shape = (batch_size, image_size, image_size, num_channels)\n",
        "  return tf.placeholder(tf.float32, shape=train_node_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m77TTK_CBbsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(images, labels, ckpt_path, dropout=False):\n",
        "  \"\"\"\n",
        "  This function contains the loop that actually trains the model.\n",
        "  :param images: a numpy array with the input data\n",
        "  :param labels: a numpy array with the output labels\n",
        "  :param ckpt_path: a path (including name) where model checkpoints are saved\n",
        "  :param dropout: Boolean, whether to use dropout or not\n",
        "  :return: True if everything went well\n",
        "  \"\"\"\n",
        "\n",
        "  # Check training data\n",
        "  assert len(images) == len(labels)\n",
        "  assert images.dtype == np.float32\n",
        "  assert labels.dtype == np.int32\n",
        "\n",
        "  # Set default TF graph\n",
        "  with tf.Graph().as_default():\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # Declare data placeholder\n",
        "    train_data_node = _input_placeholder()\n",
        "\n",
        "    # Create a placeholder to hold labels\n",
        "    train_labels_shape = (batch_size,)\n",
        "    train_labels_node = tf.placeholder(tf.int32, shape=train_labels_shape)\n",
        "\n",
        "    print(\"Done Initializing Training Placeholders\")\n",
        "\n",
        "    # Build a Graph that computes the logits predictions from the placeholder\n",
        "    if deeper:\n",
        "      logits = inference_deeper(train_data_node, dropout=dropout)\n",
        "    else:\n",
        "      logits = inference(train_data_node, dropout=dropout)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = loss_fun(logits, train_labels_node)\n",
        "#     loss_scalar = tf.summary.scalar(\"loss\",loss)\n",
        "\n",
        "    # Build a Graph that trains the model with one batch of examples and\n",
        "    # updates the model parameters.\n",
        "    train_op = train_op_fun(loss, global_step)\n",
        "\n",
        "    # Create a saver.\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "    print(\"Graph constructed and saver created\")\n",
        "\n",
        "    # Build an initialization operation to run below.\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Create and init sessions\n",
        "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) #NOLINT(long-line)\n",
        "    sess.run(init)\n",
        "    \n",
        "    writer = tf.summary.FileWriter('board_beginner')  # create writer\n",
        "    writer.add_graph(sess.graph)\n",
        "\n",
        "    print(\"Session ready, beginning training loop\")\n",
        "\n",
        "    # Initialize the number of batches\n",
        "    data_length = len(images)\n",
        "    nb_batches = math.ceil(data_length / batch_size)\n",
        "\n",
        "    for step in xrange(max_steps):\n",
        "      # for debug, save start time\n",
        "      start_time = time.time()\n",
        "\n",
        "      # Current batch number\n",
        "      batch_nb = step % nb_batches\n",
        "\n",
        "      # Current batch start and end indices\n",
        "      start, end = batch_indices(batch_nb, data_length, batch_size)\n",
        "\n",
        "      # Prepare dictionnary to feed the session with\n",
        "      feed_dict = {train_data_node: images[start:end],\n",
        "                   train_labels_node: labels[start:end]}\n",
        "\n",
        "      # Run training step\n",
        "      _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
        "      loss_scalar = tf.summary.scalar(\"loss_value\",loss_value)\n",
        "#       with teacher_summary_writer.as_default():\n",
        "#            summary.scalar('loss', loss_value, step=step)\n",
        "#       teacher_summary_writer.scalar('loss', loss_value, step=step)\n",
        "\n",
        "\n",
        "      # Compute duration of training step\n",
        "      duration = time.time() - start_time\n",
        "\n",
        "      # Sanity check\n",
        "      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
        "\n",
        "      # Echo loss once in a while\n",
        "      if step % 100 == 0:\n",
        "        sum1 = sess.run(loss_scalar, feed_dict=feed_dict)\n",
        "        writer.add_summary(sum1,step)\n",
        "        num_examples_per_step = batch_size\n",
        "        examples_per_sec = num_examples_per_step / duration\n",
        "        sec_per_batch = float(duration)\n",
        "\n",
        "        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
        "                      'sec/batch)')\n",
        "        print (format_str % (dt.now(), step, loss_value,\n",
        "                             examples_per_sec, sec_per_batch))\n",
        "\n",
        "      # Save the model checkpoint periodically.\n",
        "      if step % 1000 == 0 or (step + 1) == max_steps:\n",
        "        saver.save(sess, ckpt_path, global_step=step)\n",
        "        \n",
        "    saver.save(sess, \"model_beginner\")\n",
        "\n",
        "  return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3WzkMVl3AKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_preds(images, ckpt_path, return_logits=False):\n",
        "  \"\"\"\n",
        "  Compute softmax activations (probabilities) with the model saved in the path\n",
        "  specified as an argument\n",
        "  :param images: a np array of images\n",
        "  :param ckpt_path: a TF model checkpoint\n",
        "  :param logits: if set to True, return logits instead of probabilities\n",
        "  :return: probabilities (or logits if logits is set to True)\n",
        "  \"\"\"\n",
        "  \n",
        "  # Compute nb samples and deduce nb of batches\n",
        "  data_length = len(images)\n",
        "  nb_batches = math.ceil(len(images) / batch_size)\n",
        "\n",
        "  # Declare data placeholder\n",
        "  train_data_node = _input_placeholder()\n",
        "\n",
        "  # Build a Graph that computes the logits predictions from the placeholder\n",
        "  if deeper:\n",
        "    logits = inference_deeper(train_data_node)\n",
        "  else:\n",
        "    logits = inference(train_data_node)\n",
        "\n",
        "  if return_logits:\n",
        "    # We are returning the logits directly (no need to apply softmax)\n",
        "    output = logits\n",
        "  else:\n",
        "    # Add softmax predictions to graph: will return probabilities\n",
        "    output = tf.nn.softmax(logits)\n",
        "\n",
        "  # Restore the moving average version of the learned variables for eval.\n",
        "  variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
        "  variables_to_restore = variable_averages.variables_to_restore()\n",
        "  saver = tf.train.Saver(variables_to_restore)\n",
        "\n",
        "  # Will hold the result\n",
        "  preds = np.zeros((data_length, nb_labels), dtype=np.float32)\n",
        "\n",
        "  # Create TF session\n",
        "  with tf.Session() as sess:\n",
        "    # Restore TF session from checkpoint file\n",
        "    saver.restore(sess, ckpt_path)\n",
        "\n",
        "    # Parse data by batch\n",
        "    for batch_nb in xrange(0, int(nb_batches+1)):\n",
        "      # Compute batch start and end indices\n",
        "      start, end = batch_indices(batch_nb, data_length, batch_size)\n",
        "\n",
        "      # Prepare feed dictionary\n",
        "      feed_dict = {train_data_node: images[start:end]}\n",
        "\n",
        "      # Run session ([0] because run returns a batch with len 1st dim == 1)\n",
        "      preds[start:end, :] = sess.run([output], feed_dict=feed_dict)[0]\n",
        "\n",
        "  # Reset graph to allow multiple calls\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jonBN3Zi2Xk1",
        "colab_type": "text"
      },
      "source": [
        "# Teacher training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTHP3X4v2e8T",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokxvHeavfc9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "1317a68b-0aff-42ff-d314-155ca361643a"
      },
      "source": [
        "teacher_id = 0\n",
        "tf.summary.FileWriterCache.clear()\n",
        "import datetime\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "accuracy_scalar = tf.summary.scalar(\"accuracy\",precision)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training data: 6000\n",
            "Done Initializing Training Placeholders\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0625 02:53:19.358334 140147143387008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Graph constructed and saver created\n",
            "Session ready, beginning training loop\n",
            "2019-06-25 02:53:26.016607: step 0, loss = 8.45 (35.6 examples/sec; 3.596 sec/batch)\n",
            "2019-06-25 02:53:30.359268: step 100, loss = 7.95 (2244.2 examples/sec; 0.057 sec/batch)\n",
            "2019-06-25 02:53:35.094154: step 200, loss = 6.23 (2504.4 examples/sec; 0.051 sec/batch)\n",
            "2019-06-25 02:53:40.385384: step 300, loss = 5.66 (2313.5 examples/sec; 0.055 sec/batch)\n",
            "2019-06-25 02:53:46.277029: step 400, loss = 5.32 (2105.3 examples/sec; 0.061 sec/batch)\n",
            "2019-06-25 02:53:52.836401: step 500, loss = 5.08 (1842.8 examples/sec; 0.069 sec/batch)\n",
            "2019-06-25 02:54:00.167333: step 600, loss = 5.07 (1735.2 examples/sec; 0.074 sec/batch)\n",
            "2019-06-25 02:54:08.200480: step 700, loss = 4.68 (1540.9 examples/sec; 0.083 sec/batch)\n",
            "2019-06-25 02:54:16.807977: step 800, loss = 4.51 (1447.1 examples/sec; 0.088 sec/batch)\n",
            "2019-06-25 02:54:25.970224: step 900, loss = 5.06 (1408.6 examples/sec; 0.091 sec/batch)\n",
            "2019-06-25 02:54:35.660604: step 1000, loss = 4.24 (1294.6 examples/sec; 0.099 sec/batch)\n",
            "2019-06-25 02:54:46.373819: step 1100, loss = 4.01 (1209.2 examples/sec; 0.106 sec/batch)\n",
            "2019-06-25 02:54:57.334914: step 1200, loss = 3.85 (1127.8 examples/sec; 0.113 sec/batch)\n",
            "2019-06-25 02:55:08.838221: step 1300, loss = 3.69 (1117.3 examples/sec; 0.115 sec/batch)\n",
            "2019-06-25 02:55:20.970234: step 1400, loss = 3.55 (1048.0 examples/sec; 0.122 sec/batch)\n",
            "2019-06-25 02:55:33.634841: step 1500, loss = 3.41 (1008.6 examples/sec; 0.127 sec/batch)\n",
            "2019-06-25 02:55:46.919713: step 1600, loss = 3.27 (966.4 examples/sec; 0.132 sec/batch)\n",
            "2019-06-25 02:56:00.582211: step 1700, loss = 3.14 (910.1 examples/sec; 0.141 sec/batch)\n",
            "2019-06-25 02:56:14.853572: step 1800, loss = 3.02 (880.2 examples/sec; 0.145 sec/batch)\n",
            "2019-06-25 02:56:29.973675: step 1900, loss = 2.90 (860.6 examples/sec; 0.149 sec/batch)\n",
            "2019-06-25 02:56:45.796239: step 2000, loss = 2.79 (785.1 examples/sec; 0.163 sec/batch)\n",
            "2019-06-25 02:57:02.449962: step 2100, loss = 2.68 (780.4 examples/sec; 0.164 sec/batch)\n",
            "2019-06-25 02:57:19.210448: step 2200, loss = 2.57 (740.6 examples/sec; 0.173 sec/batch)\n",
            "2019-06-25 02:57:36.781052: step 2300, loss = 2.47 (693.4 examples/sec; 0.185 sec/batch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEf2oQ8dLMz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/board_beginner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vokjSaOP7vs6",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icAdkyw-7N3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_id = 1\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1ekuWH-74if",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIkgQYa87NRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_id = 2\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhu4neMR788G",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqFGomhz7Mmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_id = 3\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZliZISiR8A_Q",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGMg49pw7L32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_id = 4\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibdBqPw68D9L",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z2zKGFe7LOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_id = 5\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaOjsq3j8G7N",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC7tpH_p7Kj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_id = 6\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roax8U6k8KF5",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZGhi1Kz7J8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_id = 7\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuj4H9La8PVp",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNgQYu5u7JU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_id = 8\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L9qsyXz8k8F",
        "colab_type": "text"
      },
      "source": [
        "### Teacher 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCP9T_QU7IuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_id = 9\n",
        "\n",
        "# Retrieve subset of data for this teacher\n",
        "data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
        "print(\"Length of training data: \" + str(len(labels)))\n",
        "\n",
        "# Define teacher checkpoint filename and full path\n",
        "if deeper:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt'\n",
        "else:\n",
        "  filename = str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt'\n",
        "ckpt_path = train_dir + '/' + str(dataset) + '_' + filename\n",
        "\n",
        "# Perform teacher training\n",
        "assert train(data, labels, ckpt_path)\n",
        "\n",
        "# Append final step value to checkpoint for evaluation\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Retrieve teacher probability estimates on the test data\n",
        "teacher_preds = softmax_preds(test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(teacher_preds, test_labels)\n",
        "print('Precision of teacher after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nH4nzlB1WOZ",
        "colab_type": "text"
      },
      "source": [
        "# Student"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ4EjePM-xRf",
        "colab_type": "text"
      },
      "source": [
        "### Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKleBSeV-wFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def labels_from_probs(probs):\n",
        "  \"\"\"\n",
        "  Helper function: computes argmax along last dimension of array to obtain\n",
        "  labels (max prob or max logit value)\n",
        "  :param probs: numpy array where probabilities or logits are on last dimension\n",
        "  :return: array with same shape as input besides last dimension with shape 1\n",
        "          now containing the labels\n",
        "  \"\"\"\n",
        "  # Compute last axis index\n",
        "  last_axis = len(np.shape(probs)) - 1\n",
        "\n",
        "  # Label is argmax over last dimension\n",
        "  labels = np.argmax(probs, axis=last_axis)\n",
        "\n",
        "  # Return as np.int32\n",
        "  return np.asarray(labels, dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-8Y9oZp_4Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def noisy_max(logits, lap_scale, return_clean_votes=False):\n",
        "  \"\"\"\n",
        "  This aggregation mechanism takes the softmax/logit output of several models\n",
        "  resulting from inference on identical inputs and computes the noisy-max of\n",
        "  the votes for candidate classes to select a label for each sample: it\n",
        "  adds Laplacian noise to label counts and returns the most frequent label.\n",
        "  :param logits: logits or probabilities for each sample\n",
        "  :param lap_scale: scale of the Laplacian noise to be added to counts\n",
        "  :param return_clean_votes: if set to True, also returns clean votes (without\n",
        "                      Laplacian noise). This can be used to perform the\n",
        "                      privacy analysis of this aggregation mechanism.\n",
        "  :return: pair of result and (if clean_votes is set to True) the clean counts\n",
        "           for each class per sample and the original labels produced by\n",
        "           the teachers.\n",
        "  \"\"\"\n",
        "\n",
        "  # Compute labels from logits/probs and reshape array properly\n",
        "  labels = labels_from_probs(logits)\n",
        "  labels_shape = np.shape(labels)\n",
        "  labels = labels.reshape((labels_shape[0], labels_shape[1]))\n",
        "\n",
        "  # Initialize array to hold final labels\n",
        "  result = np.zeros(int(labels_shape[1]))\n",
        "\n",
        "  if return_clean_votes:\n",
        "    # Initialize array to hold clean votes for each sample\n",
        "    clean_votes = np.zeros((int(labels_shape[1]), 10))\n",
        "\n",
        "  # Parse each sample\n",
        "  for i in xrange(int(labels_shape[1])):\n",
        "    # Count number of votes assigned to each class\n",
        "    label_counts = np.bincount(labels[:, i], minlength=10)\n",
        "\n",
        "    if return_clean_votes:\n",
        "      # Store vote counts for export\n",
        "      clean_votes[i] = label_counts\n",
        "\n",
        "    # Cast in float32 to prepare before addition of Laplacian noise\n",
        "    label_counts = np.asarray(label_counts, dtype=np.float32)\n",
        "\n",
        "    # Sample independent Laplacian noise for each class\n",
        "    for item in xrange(10):\n",
        "      label_counts[item] += np.random.laplace(loc=0.0, scale=float(lap_scale))\n",
        "\n",
        "    # Result is the most frequent label\n",
        "    result[i] = np.argmax(label_counts)\n",
        "\n",
        "  # Cast labels to np.int32 for compatibility with deep_cnn.py feed dictionaries\n",
        "  result = np.asarray(result, dtype=np.int32)\n",
        "\n",
        "  if return_clean_votes:\n",
        "    # Returns several array, which are later saved:\n",
        "    # result: labels obtained from the noisy aggregation\n",
        "    # clean_votes: the number of teacher votes assigned to each sample and class\n",
        "    # labels: the labels assigned by teachers (before the noisy aggregation)\n",
        "    return result, clean_votes, labels\n",
        "  else:\n",
        "    # Only return labels resulting from noisy aggregation\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_qkrnSc_8yQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aggregation_most_frequent(logits):\n",
        "  \"\"\"\n",
        "  This aggregation mechanism takes the softmax/logit output of several models\n",
        "  resulting from inference on identical inputs and computes the most frequent\n",
        "  label. It is deterministic (no noise injection like noisy_max() above.\n",
        "  :param logits: logits or probabilities for each sample\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  # Compute labels from logits/probs and reshape array properly\n",
        "  labels = labels_from_probs(logits)\n",
        "  labels_shape = np.shape(labels)\n",
        "  labels = labels.reshape((labels_shape[0], labels_shape[1]))\n",
        "\n",
        "  # Initialize array to hold final labels\n",
        "  result = np.zeros(int(labels_shape[1]))\n",
        "\n",
        "  # Parse each sample\n",
        "  for i in xrange(int(labels_shape[1])):\n",
        "    # Count number of votes assigned to each class\n",
        "    label_counts = np.bincount(labels[:, i], minlength=10)\n",
        "\n",
        "    label_counts = np.asarray(label_counts, dtype=np.int32)\n",
        "\n",
        "    # Result is the most frequent label\n",
        "    result[i] = np.argmax(label_counts)\n",
        "\n",
        "  return np.asarray(result, dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGhvvEM4DOsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D63DOqVX-178",
        "colab_type": "text"
      },
      "source": [
        "### Student training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3gBl9PryY-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ensemble_preds(dataset, nb_teachers, stdnt_data):\n",
        "  \"\"\"\n",
        "  Given a dataset, a number of teachers, and some input data, this helper\n",
        "  function queries each teacher for predictions on the data and returns\n",
        "  all predictions in a single array. (That can then be aggregated into\n",
        "  one single prediction per input using aggregation.py (cf. function\n",
        "  prepare_student_data() below)\n",
        "  :param dataset: string corresponding to mnist, cifar10, or svhn\n",
        "  :param nb_teachers: number of teachers (in the ensemble) to learn from\n",
        "  :param stdnt_data: unlabeled student training data\n",
        "  :return: 3d array (teacher id, sample id, probability per class)\n",
        "  \"\"\"\n",
        "\n",
        "  # Compute shape of array that will hold probabilities produced by each\n",
        "  # teacher, for each training point, and each output class\n",
        "  result_shape = (nb_teachers, len(stdnt_data), nb_labels)\n",
        "\n",
        "  # Create array that will hold result\n",
        "  result = np.zeros(result_shape, dtype=np.float32)\n",
        "\n",
        "  # Get predictions from each teacher\n",
        "  for teacher_id in xrange(nb_teachers):\n",
        "    # Compute path of checkpoint file for teacher model with ID teacher_id\n",
        "    if deeper:\n",
        "      ckpt_path = teachers_dir + '/' + str(dataset) + '_' + str(nb_teachers) + '_teachers_' + str(teacher_id) + '_deep.ckpt-' + str(teachers_max_steps - 1) #NOLINT(long-line)\n",
        "    else:\n",
        "      ckpt_path = teachers_dir + '/' + str(dataset) + '_' + str(nb_teachers) + '_teachers_' + str(teacher_id) + '.ckpt-' + str(teachers_max_steps - 1)  # NOLINT(long-line)\n",
        "\n",
        "    # Get predictions on our training data and store in result array\n",
        "    result[teacher_id] = softmax_preds(stdnt_data, ckpt_path)\n",
        "\n",
        "    # This can take a while when there are a lot of teachers so output status\n",
        "    print(\"Computed Teacher \" + str(teacher_id) + \" softmax predictions\")\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYro3WOf4cwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_student_data(dataset, nb_teachers, save=False):\n",
        "  \"\"\"\n",
        "  Takes a dataset name and the size of the teacher ensemble and prepares\n",
        "  training data for the student model, according to parameters indicated\n",
        "  in flags above.\n",
        "  :param dataset: string corresponding to mnist, cifar10, or svhn\n",
        "  :param nb_teachers: number of teachers (in the ensemble) to learn from\n",
        "  :param save: if set to True, will dump student training labels predicted by\n",
        "               the ensemble of teachers (with Laplacian noise) as npy files.\n",
        "               It also dumps the clean votes for each class (without noise) and\n",
        "               the labels assigned by teachers\n",
        "  :return: pairs of (data, labels) to be used for student training and testing\n",
        "  \"\"\"\n",
        "\n",
        "  # Make sure there is data leftover to be used as a test set\n",
        "  assert stdnt_share < len(test_data)\n",
        "\n",
        "  # Prepare [unlabeled] student training data (subset of test set)\n",
        "  stdnt_data = test_data[:stdnt_share]\n",
        "\n",
        "  # Compute teacher predictions for student training data\n",
        "  teachers_preds = ensemble_preds(dataset, nb_teachers, stdnt_data)\n",
        "\n",
        "  # Aggregate teacher predictions to get student training labels\n",
        "  if not save:\n",
        "    stdnt_labels = noisy_max(teachers_preds, lap_scale)\n",
        "  else:\n",
        "    # Request clean votes and clean labels as well\n",
        "    stdnt_labels, clean_votes, labels_for_dump = noisy_max(teachers_preds, lap_scale, return_clean_votes=True) #NOLINT(long-line)\n",
        "\n",
        "    # Prepare filepath for numpy dump of clean votes\n",
        "    filepath = data_dir + \"/\" + str(dataset) + '_' + str(nb_teachers) + '_student_clean_votes_lap_' + str(lap_scale) + '.npy'  # NOLINT(long-line)\n",
        "\n",
        "    # Prepare filepath for numpy dump of clean labels\n",
        "    filepath_labels = data_dir + \"/\" + str(dataset) + '_' + str(nb_teachers) + '_teachers_labels_lap_' + str(lap_scale) + '.npy'  # NOLINT(long-line)\n",
        "\n",
        "    # Dump clean_votes array\n",
        "    with tf.gfile.Open(filepath, mode='w') as file_obj:\n",
        "      np.save(file_obj, clean_votes)\n",
        "\n",
        "    # Dump labels_for_dump array\n",
        "    with tf.gfile.Open(filepath_labels, mode='w') as file_obj:\n",
        "      np.save(file_obj, labels_for_dump)\n",
        "\n",
        "  # Print accuracy of aggregated labels\n",
        "  ac_ag_labels = accuracy(stdnt_labels, test_labels[:stdnt_share])\n",
        "  print(\"Accuracy of the aggregated labels: \" + str(ac_ag_labels))\n",
        "\n",
        "  # Store unused part of test set for use as a test set after student training\n",
        "  stdnt_test_data = test_data[stdnt_share:]\n",
        "  stdnt_test_labels = test_labels[stdnt_share:]\n",
        "\n",
        "  if save:\n",
        "    # Prepare filepath for numpy dump of labels produced by noisy aggregation\n",
        "    filepath = data_dir + \"/\" + str(dataset) + '_' + str(nb_teachers) + '_student_labels_lap_' + str(lap_scale) + '.npy' #NOLINT(long-line)\n",
        "\n",
        "    # Dump student noisy labels array\n",
        "    with tf.gfile.Open(filepath, mode='w') as file_obj:\n",
        "      np.save(file_obj, stdnt_labels)\n",
        "\n",
        "  return stdnt_data, stdnt_labels, stdnt_test_data, stdnt_test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbBe0m_34klp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call helper function to prepare student data using teacher predictions\n",
        "stdnt_dataset = prepare_student_data(dataset, nb_teachers, save=True)\n",
        "\n",
        "# Unpack the student dataset\n",
        "stdnt_data, stdnt_labels, stdnt_test_data, stdnt_test_labels = stdnt_dataset\n",
        "\n",
        "# Prepare checkpoint filename and path\n",
        "if deeper:\n",
        "  ckpt_path = train_dir + '/' + str(dataset) + '_' + str(nb_teachers) + '_student_deeper.ckpt' #NOLINT(long-line)\n",
        "else:\n",
        "  ckpt_path = train_dir + '/' + str(dataset) + '_' + str(nb_teachers) + '_student.ckpt'  # NOLINT(long-line)\n",
        "\n",
        "# Start student training\n",
        "assert train(stdnt_data, stdnt_labels, ckpt_path)\n",
        "\n",
        "# Compute final checkpoint name for student (with max number of steps)\n",
        "ckpt_path_final = ckpt_path + '-' + str(max_steps - 1)\n",
        "\n",
        "# Compute student label predictions on remaining chunk of test set\n",
        "student_preds = softmax_preds(stdnt_test_data, ckpt_path_final)\n",
        "\n",
        "# Compute teacher accuracy\n",
        "precision = accuracy(student_preds, stdnt_test_labels)\n",
        "print('Precision of student after training: ' + str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypl_vpp5Jw92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}